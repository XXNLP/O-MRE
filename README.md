O-MRE  Data
==================
Overlapping Multimodal Relation Extraction (O-MRE) Test Dataset(O-MNRE, JMERE)

This repository provides the data used in the paper:
"MHIN: A Hierarchical Interaction Network for Overlapping Multimodal Relation Extraction" (*****, 2025)

The dataset is designed to evaluate Overlapping Multimodal Relation Extraction (O-MRE) models that integrate textual and visual information. 
Each instance contains a sentence with annotated entities and relations, as well as corresponding image references from the original multimodal datasets.

------------------------------------------------------------------
Dataset Description
------------------------------------------------------------------
The test data is provided in JSON format (merged_test_data.json), where each record corresponds to a multimodal sample including its text, entity annotations, relation pairs, and associated image files.

------------------------------------------------------------------
JSON Field Descriptions
------------------------------------------------------------------
Field             | Type   | Description
------------------|--------|-----------------------------------------------------
words             | list   | Tokenized text sequence; each element represents one token of the sentence.
entity_list       | list   | List of entities. Each element contains 'name' (the entity string) and 'pos' (the start and end indices in the words sequence).
entity_pair_list  | list   | List of entity pairs and their relations. Each element is [head_index, tail_index, relation_id, sample_id], where head_index and tail_index correspond to entity indices in entity_list.
imgids            | list   | The main image filenames or paths associated with this sample. These image IDs are from the original datasets (MNRE or JMERE) released in the respective papers.
aux_imgs          | list   | Auxiliary image paths, typically generated by detection models (e.g., YOLO) in the original datasets. These images are also sourced from the original works.
rcnn_imgs         | list   | Region-cropped image paths generated by Faster-RCNN or similar detectors in the original datasets. This field also comes from the original released data.

------------------------------------------------------------------
Image Data Source
------------------------------------------------------------------
All image references (imgids, aux_imgs, and rcnn_imgs) are directly derived from the official MNRE and JMERE datasets:

- MNRE Dataset: https://github.com/thecharm/MNRE
- JMERE Dataset: https://github.com/jmre-team/JMERE

Note:
This repository does not redistribute any image files. 
Please download image data directly from the official repositories above, following their original licenses and terms of use.

------------------------------------------------------------------
Example Record
------------------------------------------------------------------
{
  "words": ["A", "new", "species", "of", "bird", "was", "found", "in", "Brazil"],
  "entity_list": [
    {"name": "species", "pos": [2, 3]},
    {"name": "Brazil", "pos": [8, 9]}
  ],
  "entity_pair_list": [
    [0, 1, "found_in", 1]
  ],
  "imgids": ["images/sample_1234.jpg"],
  "aux_imgs": ["yolo_crops/sample_1234_0.jpg"],
  "rcnn_imgs": ["rcnn_crops/sample_1234_0.jpg"]
}

------------------------------------------------------------------
Usage
------------------------------------------------------------------
You can load and inspect the dataset using Python:

import json
with open("merged_test_data.json", "r", encoding="utf-8") as f:
    data = json.load(f)
print(data[0]["words"])
print(data[0]["entity_pair_list"])

------------------------------------------------------------------
Citation
------------------------------------------------------------------
If you use this dataset, please cite:

@article{wang2025mhin,
  title={MHIN: A Hierarchical Interaction Network for Overlapping Multimodal Relation Extraction},
  author={Wang, Hailin and Ren, Hangyi and Zhang, Dan and Ma, Ao and Du, Zhekai and Liu, Guisong and Qin, Ke},
  journal={***********},
  year={2025}
}
